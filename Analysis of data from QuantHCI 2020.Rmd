---
title: "Analysis of QuantHCI 2020 data"
output: html_notebook
---

```{r setup, include = FALSE}
install.packages("jsonlite")
install.packages("gdata")
install.packages("ggplot2")
install.packages("readr")
library(jsonlite)
library(dplyr)
library(ggplot2)
library(readr)
source("R/commons.R") # load functions that are shared across notebooks
# import txt data for phrases in German and English 
phrases_en <- read_tsv("./data/phrases_en.txt")[[1]]
phrases_de <- read_tsv("./data/phrases_de.txt")[[1]]

random_en <- phrases_en[50:100]
mixed_en <- phrases_en[100:150]
sentence_en <- phrases_en[1:50]

random_de <- phrases_de[50:100]
mixed_de <- phrases_de[100:150]
sentence_de <- phrases_de[1:50]
```

The code below gives an example of how to load JSON file

```{r}
path <-"./data/04 Collected data.zip"
# unzip file and make it a list
unzip_file <- as.list(unzip(path))

# read excel file from zip
file_excel <- gdata::read.xls(unzip_file[[1]])

# Criteria to separate touch typist and non
# answered yes to any of the following: 
# "Have you taken a course to learn typing on a computer."                                         
# "Have you taught touch typing to yourself."   
criteria_1 <- which(file_excel[16] == "Yes")
criteria_2 <- which(file_excel[17] == "Yes")
touch_typist <- unique(c(criteria_1, criteria_2))

# separate excel file from list of jsons
json_file <- unzip_file[-1]

# separate touch typists and non-touch typists 
json_type <- json_file[touch_typist] 
json_non_type <- json_file[-touch_typist] 
```

These are the results for touch typists
```{r}
# get analysis for touch-typists
json_file <- json_type

ran_match <- lapply(seq_len(length(json_file)), function (z) {
  person_file <- fromJSON(json_file[[z]])
  # check if the person wrote in english or german and check how many of their entries match the entries in the txt files
  if (length(which(random_en %in% person_file$Present)) == 0) {
    length(which(random_de %in% person_file$Present))
  } else {
    length(which(random_en %in% person_file$Present))
  }
})

random_mean <- mean(unlist(ran_match))

mix_match <- lapply(seq_len(length(json_file)), function (z) {
  person_file <- fromJSON(json_file[[z]])
  # check if the person wrote in english or german and check how many of their entries match the entries in the txt files
  if (length(which(random_en %in% person_file$Present)) == 0) {
    length(which(mixed_de %in% person_file$Present))
  } else {
    length(which(mixed_en %in% person_file$Present))
  }
})

mix_mean <- mean(unlist(mix_match))

sen_match <- lapply(seq_len(length(json_file)), function (z) {
  person_file <- fromJSON(json_file[[z]])
  # check if the person wrote in english or german and check how many of their entries match the entries in the txt files
  if (length(which(random_en %in% person_file$Present)) == 0) {
    length(which(sentence_de %in% person_file$Present))
  } else {
    length(which(sentence_en %in% person_file$Present))
  }
})

sen_mean <- mean(unlist(sen_match))

data.frame("Random letters mean output" = random_mean, 
          "Mixed random letters and sentences mean output" = mix_mean,
          "Sentences mean output" = sen_mean)

# calculate IKI
# convert json files into data frames and loop over all files
# get mean IKI per participant
IKI_PP <- lapply(seq_len(length(json_file)), function (z) {
  dfs_json <- fromJSON(json_file[[z]])
  # get entries for Transcribed
  df_T <- dfs_json[[1]]
  time_diff <- lapply(seq_len(length(df_T)), function (x) {
    # isolate row in list 
    df_TP_row <- df_T[[x]]
    length_row <- length(df_TP_row) 
    # reverse sequence in length to go from the last keystroke to the first to get positive value for difference
    unlist(lapply(rev(seq_len(length_row)), function (y) {
      if ((y-1) >= 0) {
        df_T[[x]]$TimeStamp[y] - df_T[[x]]$TimeStamp[y - 1]
      }
    }))
  })
  #IKI per participant
  avg_time <- mean(unlist(time_diff))
})

# calculate average keyboard efficiency (KE) per person 
KE_PP <- lapply(seq_len(length(json_file)), function (z) {
  dfs_json <- fromJSON(json_file[[z]])
  # get entries for Transcribed
  df_T <- dfs_json[[1]]
  KE_entries <- unlist(lapply(seq_len(length(df_T)), function (x) {
    # calculate number of keysrtokes per sentence
    len_text <- length(df_T[[x]]$Text)
    # calculate total number of characters in each sentence
    characters <- nchar(df_T[[x]]$Text[[len_text]])
    KE <- characters/len_text
  }))
  mean(KE_entries)
})

# calculate average Uncorrected error (%) per person 
UER_PP <- lapply(seq_len(length(json_file)), function (z) {
  dfs_json <- jsonlite::fromJSON(json_file[[z]])
  # get mean uncorrected errors per person
  mean(unlist(as.numeric(dfs_json$UER)))
})

# average words per minute per person
wpm_PP <- lapply(seq_len(length(json_file)), function (z) {
  dfs_json <- fromJSON(json_file[[z]])
  # get total words count per person per entry
  wpm_PE <- lapply(seq_len(length(dfs_json$C)), function (x) {
    words <- dfs_json$C[[x]]
    # get time and convert in minutes
    time <- (dfs_json$Time[[x]])/(1000*60)
    wpm <- words/time
  })
  # round avg 
  wpm_avg <- round(mean(unlist(wpm_PE)))
})

# data frame with general averages for all 4 variable
data.frame("Mean wpm" = mean(unlist(wpm_PP)),
           "Mean UER" = mean(unlist(UER_PP)),
           "MEAN KE" = mean(unlist(KE_PP)),
           "MEAN IKI" = mean(unlist(IKI_PP))
)

```

These are the results for non-touch typists
```{r}
# get analysis for touch-typists
json_file <- json_non_type

ran_match <- lapply(seq_len(length(json_file)), function (z) {
  person_file <- fromJSON(json_file[[z]])
  # check if the person wrote in english or german and check how many of their entries match the entries in the txt files
  if (length(which(random_en %in% person_file$Present)) == 0) {
    length(which(random_de %in% person_file$Present))
  } else {
    length(which(random_en %in% person_file$Present))
  }
})

random_mean <- mean(unlist(ran_match))

mix_match <- lapply(seq_len(length(json_file)), function (z) {
  person_file <- fromJSON(json_file[[z]])
  # check if the person wrote in english or german and check how many of their entries match the entries in the txt files
  if (length(which(random_en %in% person_file$Present)) == 0) {
    length(which(mixed_de %in% person_file$Present))
  } else {
    length(which(mixed_en %in% person_file$Present))
  }
})

mix_mean <- mean(unlist(mix_match))

sen_match <- lapply(seq_len(length(json_file)), function (z) {
  person_file <- fromJSON(json_file[[z]])
  # check if the person wrote in english or german and check how many of their entries match the entries in the txt files
  if (length(which(random_en %in% person_file$Present)) == 0) {
    length(which(sentence_de %in% person_file$Present))
  } else {
    length(which(sentence_en %in% person_file$Present))
  }
})

sen_mean <- mean(unlist(sen_match))

data.frame("Random letters mean output" = random_mean, 
          "Mixed random letters and sentences mean output" = mix_mean,
          "Sentences mean output" = sen_mean)

# calculate IKI
# convert json files into data frames and loop over all files
# get mean IKI per participant
IKI_PP <- lapply(seq_len(length(json_file)), function (z) {
  dfs_json <- fromJSON(json_file[[z]])
  # get entries for Transcribed
  df_T <- dfs_json[[1]]
  time_diff <- lapply(seq_len(length(df_T)), function (x) {
    # isolate row in list 
    df_TP_row <- df_T[[x]]
    length_row <- length(df_TP_row) 
    # reverse sequence in length to go from the last keystroke to the first to get positive value for difference
    unlist(lapply(rev(seq_len(length_row)), function (y) {
      if ((y-1) >= 0) {
        df_T[[x]]$TimeStamp[y] - df_T[[x]]$TimeStamp[y - 1]
      }
    }))
  })
  #IKI per participant
  avg_time <- mean(unlist(time_diff))
})

# calculate average keyboard efficiency (KE) per person 
KE_PP <- lapply(seq_len(length(json_file)), function (z) {
  dfs_json <- fromJSON(json_file[[z]])
  # get entries for Transcribed
  df_T <- dfs_json[[1]]
  KE_entries <- unlist(lapply(seq_len(length(df_T)), function (x) {
    # calculate number of keysrtokes per sentence
    len_text <- length(df_T[[x]]$Text)
    # calculate total number of characters in each sentence
    characters <- nchar(df_T[[x]]$Text[[len_text]])
    KE <- characters/len_text
  }))
  mean(KE_entries)
})

# calculate average Uncorrected error (%) per person 
UER_PP <- lapply(seq_len(length(json_file)), function (z) {
  dfs_json <- jsonlite::fromJSON(json_file[[z]])
  # get mean uncorrected errors per person
  mean(unlist(as.numeric(dfs_json$UER)))
})

# average words per minute per person
wpm_PP <- lapply(seq_len(length(json_file)), function (z) {
  dfs_json <- fromJSON(json_file[[z]])
  # get total words count per person per entry
  wpm_PE <- lapply(seq_len(length(dfs_json$C)), function (x) {
    words <- dfs_json$C[[x]]
    # get time and convert in minutes
    time <- (dfs_json$Time[[x]])/(1000*60)
    wpm <- words/time
  })
  # round avg 
  wpm_avg <- round(mean(unlist(wpm_PE)))
})

# data frame with general averages for all 4 variable
data.frame("Mean wpm" = mean(unlist(wpm_PP)),
           "Mean UER" = mean(unlist(UER_PP)),
           "MEAN KE" = mean(unlist(KE_PP)),
           "MEAN IKI" = mean(unlist(IKI_PP))
)

```



```{r}
# function
.getQuantiles <- function(data) {
  quant <- quantile(unlist(data))
  quant_1 <- data[between(unlist(data), quant[1], quant[2])]
  quant_2 <- data[between(unlist(data), quant[2], quant[3])]
  quant_3 <- data[between(unlist(data), quant[3], quant[4])]
  quant_4 <- data[between(unlist(data), quant[4], quant[5])]
  data.frame("quantile" = c(quant[1], quant[2], quant[3], quant[4]),
             "frequency" = c(length(quant_1), length(quant_2), length(quant_3), length(quant_4)))
}

# additional analysis of the json data sets
# get quantiles
wpm_quant <- .getQuantiles(wpm_PP)
uer_quant <- .getQuantiles(UER_PP)
iki_quant <- .getQuantiles(IKI_PP)
ke_quant <- .getQuantiles(KE_PP)

# create plots for quantiles
ggplot(wpm_quant, aes(wpm_quant$quantile, wpm_quant$frequency)) + geom_point()
ggplot(uer_quant, aes(uer_quant$quantile, uer_quant$frequency)) + geom_point()
ggplot(iki_quant, aes(iki_quant$quantile, iki_quant$frequency)) + geom_point()
ggplot(ke_quant, aes(ke_quant$quantile, ke_quant$frequency)) + geom_point()
```